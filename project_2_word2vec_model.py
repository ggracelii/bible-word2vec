# -*- coding: utf-8 -*-
"""Project 2 - Word2vec Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IC-sO2Jy4qd9-Nz2Ji3KjKAengnDvKza

# ***Run in Google Colab***

##### 1. Load a corpus to use as the basis for your word2vec model. You can choose any of the ones that are available with NLTK, or something that we've used in class (for example, the Yelp reviews dataset).
"""

# import stuff and download stuff
from google.colab import drive
drive.mount('/content/drive')
from gensim import models
from gensim.models import KeyedVectors
import nltk.corpus
nltk.download('punkt_tab')
nltk.download('inaugural')
nltk.download('gutenberg')

# get Bible text
text = nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids()[3])

"""##### 2. Pre-process the corpus as you see fit (for example, filter out non-English entries, lowercase, remove punctuation and/or stopwords, etc.)"""

from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

import string
from nltk.tokenize import sent_tokenize, word_tokenize

# split text into phrases
def get_phrases(text):
    words = nltk.tokenize.word_tokenize(text)
    phrases = {}
    current_phrase = []
    for word in words:
        if (word in stop_words or word in string.punctuation):
            if (len(current_phrase) > 1):
                phrases[" ".join(current_phrase)] = "_".join(current_phrase)
                current_phrase = []
        else:
            current_phrase.append(word)

    if (len(current_phrase) > 1):
        phrases[" ".join(current_phrase)] = "_".join(current_phrase)
    return phrases

# replae phrases with vocab
def replace_phrases(phrases_dict, text):
    for phrase in phrases_dict.keys():
        text = text.replace(phrase, phrases_dict[phrase])
    return text

# clean + tokenize data
import regex as re

text = re.sub(r'\d+', '', text)
phrases = get_phrases(text)
text = replace_phrases(phrases, text)
sentences = sent_tokenize(text)
words = [word_tokenize(sentence.lower()) for sentence in sentences]
model = models.Word2Vec(words, min_count=1)

"""##### 3. Train the word2vec model and save it."""

model.train(words, total_examples=model.corpus_count, epochs=400)
model.save("/content/drive/MyDrive/Year 1/Fall Semester/4 Computing in Linguistics/Projects/word2vec_model.model")

"""##### 4. Evaluate the model using analogies."""

question_words = "/content/drive/MyDrive/Year 1/Fall Semester/4 Computing in Linguistics/Projects/question_words.txt"
score, sections = model.wv.evaluate_word_analogies(question_words)
print(score)
# print(sections[-1])
# not very accurate! makes sense as the Bible does not cover a lot of English vocab, so it is bad at general analogies

"""##### 5. Load the Google word2vec model."""

google_model = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/Year 1/Fall Semester/4 Computing in Linguistics/Projects/GoogleNews-vectors-negative300.bin.gz", binary=True, limit=20000)

"""##### 6. Compare most similar words to several words of your choosing between the two models. Suggestions: "man", "woman", "apple", "dinner", "pen"."""

words = ["man", "woman", "father", "mother", "life", "faith", "fruit"]

for word in words:
  model_similar_words = model.wv.most_similar(word, topn=10)
  google_similar_words = google_model.most_similar(word, topn=10)
  print(word,"\n    Model:\n\t", model_similar_words, "\n    Google:\n\t", google_similar_words, "\n")

"""##### 7. Compare most similar words to the same words, but removing some of the contexts (for example, removing the vector for "man" when calculating the most similar words for "woman")."""

words = [["man", "woman"], ["woman", "man"], ["father", "mother"], ["mother", "father"], ["life", "faith"]]

for pair in words:
  model_similar_words = model.wv.most_similar(positive=pair[0], negative=pair[1], topn=10)
  google_similar_words = google_model.most_similar(positive=pair[0], negative=pair[1], topn=10)
  print("Positive:", pair[0], "  Negative:", pair[1], "\n    Model:\n\t", model_similar_words, "\n    Google:\n\t", google_similar_words, "\n")

"""##### 8. What are some differences between the two models and how can you explain them?

The most similar words for my bible model is vastly different for positive="life", negative="faith" than the most similar words for just "life" whereas the Google model doesn't really change. This exemplifies the emphasis the bible places on faith in relation to life. With faith, life is similar to wisdom, heart, hope, soul, and sins. Without faith, life is reduced to less siginificant ideas, such as "ephraim_shall_return" and "farm".

Additionally, the most similar words for "man" without "woman" and "woman" without "man" for my bible model tend to be more abstract words that don't really connect to the ideas of man and woman, such as "work_therein", "hath_walked", "yellow_gold", and "weavest". This is also the case for "mother" and "father": "receieved_sight", "laid_incense_thereon", "but_oh", and "light_affliction" are among the most similar words. This phenomenon indicates that my bible model is not precise for analogous pairs and context-specific semantic relationships, whereas the Google model can more accurately capture the relationship between people, such as man/woman or mother/father.

Also, my bible model tends to have more phrases in the most similar words. This could be due to the model struggling with abstract analogies or archaic language, whereas the Google model showcases more colloquial language that is common in modern English.
"""